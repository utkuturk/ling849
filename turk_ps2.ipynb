{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "41Ey2m5m0Sv5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fname_train = \"training1.pt\"\n",
        "fname_class = \"classes.pt\""
      ],
      "metadata": {
        "id": "D-ldWjquS1qv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = torch.load(fname_train)\n",
        "classes = torch.load(fname_class)\n",
        "training_dataset = torch.utils.data.TensorDataset(training, classes)\n",
        "train_loader = torch.utils.data.DataLoader(training_dataset, shuffle=True)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(20, 10),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(10, 7),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    for i, (input, label) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqHLAGXE1Msi",
        "outputId": "2cd6da0d-fb9d-46c8-8e6f-7477efcd3aaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-4c4c8a500ddf>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  training = torch.load(fname_train)\n",
            "<ipython-input-3-4c4c8a500ddf>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  classes = torch.load(fname_class)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2."
      ],
      "metadata": {
        "id": "Olk435d-2sm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname_test = \"test.pt\"\n",
        "test = torch.load(fname_test)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14xzD7Ve2Q2C",
        "outputId": "3f2d7b49-15b5-46f1-89ee-daebe75c8f55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-e6eb8599173b>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test = torch.load(fname_test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3."
      ],
      "metadata": {
        "id": "muptTAIe7E3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def germanplural(output_activations):\n",
        "    # most informative of the row\n",
        "    _, predicted_classes = torch.max(output_activations, dim=1)\n",
        "    # numpy it\n",
        "    predicted_classes = predicted_classes.numpy()\n",
        "    # bare confusion matrix\n",
        "    confusion_matrix = np.zeros((8, 7), dtype=int)\n",
        "    # recreate test data organization to pass to confusion matrix\n",
        "    model_classes = []\n",
        "    # 10 from each class\n",
        "    for i in range(7):\n",
        "        model_classes.extend([i] * 10)\n",
        "    # additional 100 for the last test set\n",
        "    model_classes.extend([7] * 100)\n",
        "    model_classes = np.array(model_classes)\n",
        "    # for each class in model class\n",
        "    # put them to a row according to the test data structure\n",
        "    # put them to a column according to the prediction.\n",
        "    for i in range(len(model_classes)):\n",
        "        cur_class = model_classes[i]\n",
        "        predicted_class = predicted_classes[i]\n",
        "        if cur_class < 7:\n",
        "            confusion_matrix[cur_class, predicted_class] += 1\n",
        "        else:\n",
        "            confusion_matrix[7, predicted_class] += 1\n",
        "    return confusion_matrix"
      ],
      "metadata": {
        "id": "vjuA46WW2xuG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "germanplural(outputs)\n",
        "# Here's every row is trying to replicate test data structure.\n",
        "# row 1 is class 1 prototypes, row 2 is class 2 prototypes.\n",
        "# columns show how many in that column predicted to be class 1/2/3, etc.\n",
        "# The column number reflects the predicted class number."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUn00U-kUwFw",
        "outputId": "0bff4e0a-9be0-44e4-fcaa-90364c7e92ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 9,  0,  0,  1,  0,  0,  0],\n",
              "       [ 0, 10,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0, 10,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0, 10,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 10,  0,  0],\n",
              "       [ 0,  1,  0,  0,  0,  7,  2],\n",
              "       [ 0,  0,  0,  0,  0,  0, 10],\n",
              "       [ 9, 11, 14, 12, 22, 12, 20]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4."
      ],
      "metadata": {
        "id": "jCzHBBxr7Pib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fname_train = \"training2.pt\"\n",
        "fname_class = \"classes.pt\"\n",
        "fname_test = \"test.pt\"\n",
        "training = torch.load(fname_train)\n",
        "classes = torch.load(fname_class)\n",
        "training_dataset = torch.utils.data.TensorDataset(training, classes)\n",
        "train_loader = torch.utils.data.DataLoader(training_dataset, shuffle=True)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(20, 10),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(10, 7),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    for i, (input, label) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "test = torch.load(fname_test)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLhTiRZb7IUk",
        "outputId": "330e7684-e840-4d48-ace2-6eeac5df32e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ae524a1be451>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  training = torch.load(fname_train)\n",
            "<ipython-input-7-ae524a1be451>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  classes = torch.load(fname_class)\n",
            "<ipython-input-7-ae524a1be451>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test = torch.load(fname_test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(germanplural(outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o-J28srZ6-o",
        "outputId": "619d5b24-5cb2-40d2-cc6c-45b54840d67d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4  0  6  0  0  0  0]\n",
            " [ 0 10  0  0  0  0  0]\n",
            " [ 0  0 10  0  0  0  0]\n",
            " [ 0  0  0 10  0  0  0]\n",
            " [ 0  0  0  0 10  0  0]\n",
            " [ 3  1  0  1  0  0  5]\n",
            " [ 0  0  0  0  0  0 10]\n",
            " [55  4  9  7 14  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.\n",
        "Under which conditions would you expect a general purpose statistical learning mechanism to be able to acquire a minority default?\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Answer**:\n",
        "Given the results, I would expect general models like the one we trained to acquire a minority default in cases when the input has the inherent characteristics of randomness. The training data or its manipulated version should be such that there are no generalizations to be drawn from the minority default case.\n",
        "\n",
        "<br/>\n",
        "\n",
        "*Personal Thoughts*: This does not make a lot of sense to me. There is no way that German speakers or any other minority default-type language speaker just hear random sounds."
      ],
      "metadata": {
        "id": "BwIIgC-a90-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.\n",
        "Given that German has a minority default, what prediction would you make about the structure of the German lexicon?\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Answer**: My hypothesis would be that the german lexicon and word formation is not based on analogy/comparison. The storage probably is not optimized for analogy or prototypicality relations. However, I feel like analogy still has a place in it, given the examples you have provided in the problem set pdf. It would be fair to say, even though analogy is inescapable, rule based connections is a must.\n"
      ],
      "metadata": {
        "id": "Tv8Wyik2CJ-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.\n",
        "\n",
        "Propose one other way in which the input data to the network could be manipulated, and make a prediction about how your proposed manipulation will affect learning performance in the network.\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Answer**: Given the German language and the demographic of Germany, I feel like kids in Germany hear a lot of different words and sounds from minority languages like Turkish, Polish, etc. If one assumes that a kid parser deterministically think about or generates possible word forms as they hear or entertain their plural forms in addition to the ones in German, their training data's first row should look like something like the combination of training1 and training2.\n",
        "\n",
        "In addition to this, there should be probably some noise in that training data as well, as well as their mapping.\n",
        "\n",
        "Thirdly, the contexts those -s on Turkish words and German words, as well as other German endings, are going to be different. So, it would be smart to add some sort of feature weighing as well.\n",
        "\n",
        "Lastly, piggybacking on the feature weighing, we can implement a learned bias towards -s. Maybe, after seeing bunch of random stuff, the evidence needed for other classes is pretty low, and the when a word cannot be placed in those classes, a german kid automatically choses -s class. In this modelling exercise, I am putting weights that I just come up with, but in a better model, one can do that using other values from a speech corpus, maybe like phonemic surprisal?"
      ],
      "metadata": {
        "id": "Qgo01fKtC5ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8."
      ],
      "metadata": {
        "id": "ENognZHsFA0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine trainign1 and trainign2 first class fifty fifty\n",
        "# I tried a lot with adding noise and randomly given contextual weights. But I was not able to do it properly.\n",
        "# I think I almost did it with flags, but I failed.\n",
        "# Thus, I am giving something that works, rather than that.\n",
        "# I am sorry.\n",
        "\n",
        "# But I was able to implement the feature weighing for the different classes.\n",
        "\n",
        "fname_train1 = \"training1.pt\"\n",
        "fname_train2 = \"training2.pt\"\n",
        "fname_class = \"classes.pt\"\n",
        "\n",
        "training1 = torch.load(fname_train1)\n",
        "classes = torch.load(fname_class)\n",
        "training2 = torch.load(fname_train2)\n",
        "\n",
        "dataset1 = torch.utils.data.TensorDataset(training1, classes)\n",
        "dataset2 = torch.utils.data.TensorDataset(training2, classes)\n",
        "\n",
        "# separate class1s\n",
        "class1_indices_dataset1 = (classes[:, 0] == 1).nonzero().squeeze()\n",
        "class1_indices_dataset2 = (classes[:, 0] == 1).nonzero().squeeze()\n",
        "class1_dataset1 = torch.utils.data.TensorDataset(training1[class1_indices_dataset1], classes[class1_indices_dataset1])\n",
        "class1_dataset2 = torch.utils.data.TensorDataset(training2[class1_indices_dataset2], classes[class1_indices_dataset2])\n",
        "half_class1_size = len(class1_dataset1) // 2\n",
        "\n",
        "# get half of the class1s from different datasets\n",
        "class1_half_dataset1 = torch.utils.data.TensorDataset(class1_dataset1.tensors[0][:half_class1_size], class1_dataset1.tensors[1][:half_class1_size])\n",
        "class1_half_dataset2 = torch.utils.data.TensorDataset(class1_dataset2.tensors[0][:half_class1_size], class1_dataset2.tensors[1][:half_class1_size])\n",
        "\n",
        "# put them together\n",
        "combined_class1 = torch.utils.data.TensorDataset(\n",
        "    torch.cat((class1_half_dataset1.tensors[0], class1_half_dataset2.tensors[0])),\n",
        "    torch.cat((class1_half_dataset1.tensors[1], class1_half_dataset2.tensors[1]))\n",
        ")\n",
        "\n",
        "# get the non-class1 parts\n",
        "non_class1_indices = (classes[:, 0] == 0).nonzero().squeeze()\n",
        "non_class1_dataset = torch.utils.data.TensorDataset(training1[non_class1_indices], classes[non_class1_indices])\n",
        "final_training_data = torch.cat((combined_class1.tensors[0], non_class1_dataset.tensors[0]))\n",
        "final_classes = torch.cat((combined_class1.tensors[1], non_class1_dataset.tensors[1]))\n",
        "\n",
        "# final dataset\n",
        "final_dataset = torch.utils.data.TensorDataset(final_training_data, final_classes)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mThg7QPhBJ5z",
        "outputId": "4bb834ae-3aa8-44d5-e330-d0b23721a493"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-3fe6d0258a3d>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  training1 = torch.load(fname_train1)\n",
            "<ipython-input-9-3fe6d0258a3d>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  classes = torch.load(fname_class)\n",
            "<ipython-input-9-3fe6d0258a3d>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  training2 = torch.load(fname_train2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fname_test = \"test.pt\"\n",
        "\n",
        "training_dataset = final_dataset\n",
        "train_loader = torch.utils.data.DataLoader(training_dataset, shuffle=True)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(20, 10),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(10, 7),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        weights = torch.ones(7)\n",
        "        weights[0] = 5.5\n",
        "        weights[1:] *= 0.8\n",
        "        weighted_criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "        loss = weighted_criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "test = torch.load(fname_test)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvMaaH3QB82R",
        "outputId": "440b7a43-c3c4-44ce-a99b-c0c362a8789f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-75b9c51b70da>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test = torch.load(fname_test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(germanplural(outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkbskmybCrNV",
        "outputId": "95f5514e-a55b-49fd-d42e-e247c96d795d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[10  0  0  0  0  0  0]\n",
            " [ 0 10  0  0  0  0  0]\n",
            " [ 0  0 10  0  0  0  0]\n",
            " [ 1  0  0  9  0  0  0]\n",
            " [ 0  0  0  0 10  0  0]\n",
            " [ 3  3  0  2  2  0  0]\n",
            " [10  0  0  0  0  0  0]\n",
            " [70  6  9  9  6  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer to 8**: Even though I was not able to implement the ability to track random-class-1s and prototypical class-1s as a way to implement contextual feature weighing, even the simple feature weighing of classes made the model slightly better.\n",
        "\n",
        "The only problem now is that I am not able to predict the last 2 classes correctly. The antepenultimate class was always bad in all models. Yet the penultimate class is messed up simply due to my changes. I assume that slightly more data would fix it? I tried more data, which was just concetanating two datasets, instead of 50/50. Now, the results are better."
      ],
      "metadata": {
        "id": "iYXpX09kFW3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fname_train1 = \"training1.pt\"\n",
        "fname_train2 = \"training2.pt\"\n",
        "fname_class = \"classes.pt\"\n",
        "fname_test = \"test.pt\"\n",
        "\n",
        "training1 = torch.load(fname_train1)\n",
        "classes = torch.load(fname_class)\n",
        "training2 = torch.load(fname_train2)\n",
        "\n",
        "dataset1 = torch.utils.data.TensorDataset(training1, classes)\n",
        "dataset2 = torch.utils.data.TensorDataset(training2, classes)\n",
        "\n",
        "combined_training_data = torch.cat((training1, training2), dim=0)\n",
        "combined_classes = torch.cat((classes, classes), dim=0)\n",
        "combined_dataset = torch.utils.data.TensorDataset(combined_training_data, combined_classes)\n",
        "\n",
        "training_dataset = combined_dataset\n",
        "train_loader = torch.utils.data.DataLoader(training_dataset, shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(20, 10),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(10, 7),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        weights = torch.ones(7)\n",
        "        weights[0] = 3.0\n",
        "        weights[1:] *= 1.0\n",
        "\n",
        "        weighted_criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "        loss = weighted_criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "test = torch.load(fname_test)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9RLDSJzKEte",
        "outputId": "117cfe9f-5ca6-414a-c2a6-792dfb72cd8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-6133ee59f11c>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  training1 = torch.load(fname_train1)\n",
            "<ipython-input-12-6133ee59f11c>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  classes = torch.load(fname_class)\n",
            "<ipython-input-12-6133ee59f11c>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  training2 = torch.load(fname_train2)\n",
            "<ipython-input-12-6133ee59f11c>:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test = torch.load(fname_test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "germanplural(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N9QKa16KHUk",
        "outputId": "333790b0-373f-448a-a134-91692a2049ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 10,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0, 10,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0, 10,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 10,  0,  0],\n",
              "       [ 3,  3,  0,  0,  0,  0,  4],\n",
              "       [ 0,  0,  0,  0,  0,  0, 10],\n",
              "       [64,  7,  7,  6,  9,  0,  7]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}